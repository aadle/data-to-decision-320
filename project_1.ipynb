{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "- Dokumentere bruk av KI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pymongo.errors\n",
    "import plotly.express as px\n",
    "import uuid\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up remote database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. Successfully connected to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "with open(\"../.nosync/mongoDB.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "uri = (\n",
    "    \"mongodb+srv://medvetslos:\"\n",
    "    + json.load(open(\"../.nosync/mongoDB.json\"))[\"pwd\"]\n",
    "    + \"@ind320-project.lunku.mongodb.net/?retryWrites=true&w=majority&appName=IND320-project\"\n",
    ")\n",
    "\n",
    "mdb_client = MongoClient(uri, server_api=ServerApi(\"1\"))\n",
    "\n",
    "try:\n",
    "    mdb_client.admin.command(\"ping\")\n",
    "    print(\"Pinged your deployment. Successfully connected to MongoDB.\")\n",
    "except Exception as exceptionMsg:\n",
    "    print(exceptionMsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'muncipalities' already exists.\n",
      "Collection 'gas' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Creating collections for municipality data and gas prices\n",
    "database = mdb_client[\"IND320-project\"]\n",
    "collection_names = [\"muncipalities\", \"gas\"]\n",
    "\n",
    "for name in collection_names:\n",
    "    # Checking if collection exists. If not, create the collection.\n",
    "    try:\n",
    "        database.create_collection(name)\n",
    "        print(f\"Collection '{name}' was created successfully.\")\n",
    "    except pymongo.errors.CollectionInvalid:\n",
    "        print(f\"Collection '{name}' already exists.\")\n",
    "\n",
    "municipalities = database[\"municipalities\"]\n",
    "gas = database[\"gas\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster([\"localhost\"], port=9042)\n",
    "session = cluster.connect()\n",
    "keyspace = \"ind320_project\"\n",
    "session.execute(\n",
    "    \"CREATE KEYSPACE IF NOT EXISTS\" + \n",
    "    \" \" + \n",
    "    keyspace + \n",
    "    \" \" + \n",
    "    \"WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1};\"\n",
    ")\n",
    "\n",
    "session.set_keyspace(keyspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data to be inserted into the collection.\n"
     ]
    }
   ],
   "source": [
    "webscrape_url = \"https://en.wikipedia.org/wiki/List_of_municipalities_of_Denmark\"\n",
    "\n",
    "page = requests.get(webscrape_url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "wiki_table = soup.find(\"table\", attrs={\"class\": \"wikitable sortable\"})\n",
    "\n",
    "df_municipalities = pd.read_html(StringIO(str(wiki_table)))[0]\n",
    "records_municipalities = df_municipalities.to_dict(\"records\")\n",
    "\n",
    "LAU_1_code = df_municipalities.columns.tolist()[0]\n",
    "\n",
    "\n",
    "# check if data we are writing already exists\n",
    "existing_entries = list(\n",
    "    database[\"municipalities\"].find(\n",
    "        {\n",
    "            LAU_1_code: {\n",
    "                \"$in\": [record[LAU_1_code] for record in records_municipalities]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# if new data,\n",
    "new_entries = [\n",
    "    entry for entry in records_municipalities\n",
    "    if not any(existing_entry[LAU_1_code] == entry[LAU_1_code] for existing_entry in existing_entries)\n",
    "]\n",
    "\n",
    "# Writing to MongoDB\n",
    "if len(new_entries) > 0:\n",
    "    database[\"municipalities\"].insert_many(new_entries)\n",
    "    print(\"Data successfully written into the collection.\")\n",
    "else:\n",
    "    print(\"No new data to be inserted into the collection.\")\n",
    "\n",
    "# database[\"municipalities\"].delete_many({}) # delete all records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.energidataservice.dk/dataset/\"\n",
    "filtering = \"?offset=0&start=2022-01-01T00:00&end=2023-01-01T00:00&sort=HourUTC%20DESC\"\n",
    "\n",
    "api_datasets = {\n",
    "    \"remote\": {\"GasDailyBalancingPrice\": \"gas\"},\n",
    "    \"local\": {\n",
    "         \"production\": \"ProductionMunicipalityHour\", \n",
    "         \"consumption\": \"ConsumptionIndustry\",\n",
    "         \"prodcons\": \"ProductionConsumptionSettlement\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(dataset: str):\n",
    "    return requests.get(api_url + dataset + filtering).json()[\"records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_production = pd.DataFrame.from_records(get_json_data(api_datasets[\"local\"][\"production\"]))\n",
    "df_consumption = pd.DataFrame.from_records(get_json_data(api_datasets[\"local\"][\"consumption\"]))\n",
    "df_prodcons = pd.DataFrame.from_records(get_json_data(api_datasets[\"local\"][\"prodcons\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HourUTC                     object\n",
       "HourDK                      object\n",
       "MunicipalityNo              object\n",
       "SolarMWh                   float64\n",
       "OffshoreWindLt100MW_MWh    float64\n",
       "OffshoreWindGe100MW_MWh    float64\n",
       "OnshoreWindMWh             float64\n",
       "ThermalPowerMWh            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_production.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_production['HourDK'] = pd.to_datetime(df_production['HourDK'])\n",
    "df_production['HourUTC'] = pd.to_datetime(df_production['HourUTC'])\n",
    "df_production['MunicipalityNo'] = df_production['MunicipalityNo'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HourUTC            object\n",
       "HourDK             object\n",
       "MunicipalityNo     object\n",
       "Branche            object\n",
       "ConsumptionkWh    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_consumption.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption['HourDK'] = pd.to_datetime(df_consumption['HourDK'])\n",
    "df_consumption['HourUTC'] = pd.to_datetime(df_consumption['HourUTC'])\n",
    "df_consumption['MunicipalityNo'] = df_consumption['MunicipalityNo'].astype(int)\n",
    "df_consumption['Branche'] = df_consumption['HourUTC'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HourUTC                        object\n",
       "HourDK                         object\n",
       "PriceArea                      object\n",
       "CentralPowerMWh               float64\n",
       "LocalPowerMWh                 float64\n",
       "CommercialPowerMWh            float64\n",
       "LocalPowerSelfConMWh          float64\n",
       "OffshoreWindLt100MW_MWh       float64\n",
       "OffshoreWindGe100MW_MWh       float64\n",
       "OnshoreWindLt50kW_MWh         float64\n",
       "OnshoreWindGe50kW_MWh         float64\n",
       "HydroPowerMWh                 float64\n",
       "SolarPowerLt10kW_MWh          float64\n",
       "SolarPowerGe10Lt40kW_MWh      float64\n",
       "SolarPowerGe40kW_MWh          float64\n",
       "SolarPowerSelfConMWh          float64\n",
       "UnknownProdMWh                float64\n",
       "ExchangeNO_MWh                float64\n",
       "ExchangeSE_MWh                float64\n",
       "ExchangeGE_MWh                float64\n",
       "ExchangeNL_MWh                float64\n",
       "ExchangeGB_MWh                 object\n",
       "ExchangeGreatBelt_MWh         float64\n",
       "GrossConsumptionMWh           float64\n",
       "GridLossTransmissionMWh       float64\n",
       "GridLossInterconnectorsMWh    float64\n",
       "GridLossDistributionMWh       float64\n",
       "PowerToHeatMWh                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prodcons.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prodcons['HourDK'] = pd.to_datetime(df_prodcons['HourDK'])\n",
    "df_prodcons['HourUTC'] = pd.to_datetime(df_prodcons['HourUTC'])\n",
    "df_prodcons['PriceArea'] = df_prodcons['PriceArea'].astype(str)\n",
    "df_prodcons['ExchangeGB_MWh'] = df_prodcons[\"ExchangeGB_MWh\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prodcons = df_prodcons.rename(columns=str.lower)\n",
    "df_production = df_production.rename(columns=str.lower)\n",
    "df_consumption = df_consumption.rename(columns=str.lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt: Can you write me a function which makes the table creation query from a \n",
    "#         Pandas DataFrame which assigns the correct datatype to the Cassandra table.\n",
    "#         The primary key should be named id and be of type timeuuid\n",
    "def create_cassandra_table_query(df, keyspace, table_name):\n",
    "    # Define mapping between Pandas and Cassandra datatypes\n",
    "    dtype_mapping = {\n",
    "        'int64': 'int',\n",
    "        'float64': 'double',\n",
    "        'object': 'text',\n",
    "        'bool': 'boolean',\n",
    "        'datetime64[ns]': 'timestamp'\n",
    "    }\n",
    "    \n",
    "    # Start constructing the CREATE TABLE query\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} (\\n\"\n",
    "    \n",
    "    # Add primary key column with timeuuid\n",
    "    columns = [\"id timeuuid\"]\n",
    "    \n",
    "    # Add remaining columns with mapped Cassandra data types\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if col != \"id\":  # Exclude 'id' to avoid duplication\n",
    "            cassandra_type = dtype_mapping.get(str(dtype), 'text')  # Default to 'text' if type is unrecognized\n",
    "            columns.append(f\"{col} {cassandra_type}\")\n",
    "    \n",
    "    # Join columns with commas and specify primary key as 'id'\n",
    "    columns_str = \",\\n    \".join(columns)\n",
    "    query += f\"    {columns_str},\\n\"\n",
    "    query += f\"    PRIMARY KEY (id)\\n);\"\n",
    "    \n",
    "    return query\n",
    "\n",
    "# def create_cassandra_table_query(df, keyspace, table_name, primary_key):\n",
    "#     # Define mapping between Pandas and Cassandra datatypes\n",
    "#     dtype_mapping = {\n",
    "#         'int64': 'int',\n",
    "#         'float64': 'double',\n",
    "#         'object': 'text',\n",
    "#         'bool': 'boolean',\n",
    "#         'datetime64[ns]': 'timestamp'\n",
    "#     }\n",
    "    \n",
    "#     # Start constructing the CREATE TABLE query\n",
    "#     query = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} (\\n\"\n",
    "    \n",
    "#     # Add each column name and its corresponding Cassandra data type\n",
    "#     columns = []\n",
    "#     for col, dtype in df.dtypes.items():\n",
    "#         cassandra_type = dtype_mapping.get(str(dtype), 'text')  # Default to 'text' if type is unrecognized\n",
    "#         columns.append(f\"{col} {cassandra_type}\")\n",
    "    \n",
    "#     # Join columns with commas and specify primary key\n",
    "#     columns_str = \",\\n    \".join(columns)\n",
    "#     query += f\"    {columns_str},\\n\"\n",
    "#     query += f\"    PRIMARY KEY ({primary_key})\\n);\"\n",
    "    \n",
    "#     return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_table = create_cassandra_table_query(df_production, keyspace, \"production\")\n",
    "consumption_table = create_cassandra_table_query(df_consumption, keyspace, \"consumption\") \n",
    "prodcons_table = create_cassandra_table_query(df_prodcons, keyspace, \"prodcons\") \n",
    "\n",
    "for table_query in [production_table, consumption_table, prodcons_table]:\n",
    "    session.execute(table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in keyspace 'ind320_project':\n",
      "consumption\n",
      "prodcons\n",
      "production\n"
     ]
    }
   ],
   "source": [
    "query = f\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'ind320_project'\"\n",
    "keyspace = \"ind320_project\"\n",
    "# Execute the query\n",
    "rows = session.execute(query)\n",
    "\n",
    "# Print the table names\n",
    "print(f\"Tables in keyspace '{keyspace}':\")\n",
    "for row in rows:\n",
    "    print(row.table_name)\n",
    "    # session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{row.table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" \n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 21:37:38 WARN Utils: Your hostname, Aarons-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.11.132 instead (on interface en0)\n",
      "24/10/07 21:37:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/aaron/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/aaron/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f34dfb06-2a06-4fc6-aa27-213c7d040687;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/aaron/Documents/IND320_projects/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 258ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f34dfb06-2a06-4fc6-aa27-213c7d040687\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/6ms)\n",
      "24/10/07 21:37:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config(\"spark.driver.memory\", \"4g\").\\\n",
    "    config(\"spark.executor.memory\", \"4g\").\\\n",
    "    config(\"spark.task.maxFailures\", \"10\").\\\n",
    "    config(\"spark.sql.shuffle.partitions\", \"200\").\\\n",
    "    getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 21:43:33 WARN TaskSetManager: Stage 3 contains a task of very large size (5307 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def generate_timeuuid():\n",
    "    return str(uuid.uuid1())\n",
    "\n",
    "timeuuid_udf = udf(generate_timeuuid, StringType())\n",
    "\n",
    "spark.createDataFrame(df_production).withColumn(\"id\", timeuuid_udf())\\\n",
    "    .write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .options(table=\"production\", keyspace=\"ind320_project\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/07 21:44:35 WARN TaskSetManager: Stage 4 contains a task of very large size (12495 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_consumption).withColumn(\"id\", timeuuid_udf())\\\n",
    "    .write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .options(table=\"consumption\", keyspace=\"ind320_project\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_prodcons).withColumn(\"id\", timeuuid_udf())\\\n",
    "    .write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .options(table=\"prodcons\", keyspace=\"ind320_project\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea now:\n",
    "- Use Spark/Cassandra to retrieve data rather than through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in keyspace 'ind320_project':\n",
      "consumption\n",
      "prodcons\n",
      "production\n"
     ]
    }
   ],
   "source": [
    "query = f\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'ind320_project'\"\n",
    "keyspace = \"ind320_project\"\n",
    "# Execute the query\n",
    "rows = session.execute(query)\n",
    "\n",
    "# Print the table names\n",
    "print(f\"Tables in keyspace '{keyspace}':\")\n",
    "for row in rows:\n",
    "    print(row.table_name)\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{row.table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
